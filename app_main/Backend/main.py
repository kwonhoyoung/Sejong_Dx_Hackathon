"""
Claudeì™€ Perplexityë¥¼ í™œìš©í•œ ì§€ëŠ¥í˜• ì´ìŠˆ ê²€ìƒ‰ ì‹œìŠ¤í…œ (ê¸°ê°„ ì§€ì • ê¸°ëŠ¥ ê°œì„  ë²„ì „)
- í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰, LLM-as-a-judge ê²€ì¦, AI ë¶„ì„ ê¸°ëŠ¥ í¬í•¨
"""

import asyncio
import re
import time
from typing import List, Dict, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta  # ì¶”ê°€ëœ ë¶€ë¶„
import httpx
from loguru import logger
import anthropic
import os
from dotenv import load_dotenv
import sys

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
load_dotenv()


# --- ë°ì´í„° ëª¨ë¸ ì •ì˜ ---

@dataclass
class VerificationResult:
    """LLM íŒì‚¬ì˜ ê²€ì¦ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""
    status: str = "ë¯¸ê²€ì¦"  # "VERIFIED", "UNVERIFIED", "NEEDS_REVIEW" ì¤‘ í•˜ë‚˜ì˜ ìƒíƒœë¥¼ ê°€ì§‘ë‹ˆë‹¤.
    reasoning: str = "ì•„ì§ ê²€ì¦ì´ ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

@dataclass
class IssueItem:
    """ê°œë³„ ì´ìŠˆ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""
    title: str
    summary: str
    source: str
    published_date: Optional[str]
    relevance_score: float
    category: str
    url: Optional[str] = None
    content: Optional[str] = None
    verification: VerificationResult = field(default_factory=VerificationResult) # ì´ìŠˆ ê²€ì¦ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

@dataclass
class SearchResult:
    """ìµœì¢… ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ëŠ” ë°ì´í„° í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""
    topic: str
    keywords: List[str]
    issues: List[IssueItem]
    initial_found: int
    verified_found: int
    search_time: float
    search_start_date: str  # ì¶”ê°€ëœ ë¶€ë¶„: ê²€ìƒ‰ ì‹œì‘ì¼
    search_end_date: str    # ì¶”ê°€ëœ ë¶€ë¶„: ê²€ìƒ‰ ì¢…ë£Œì¼


# --- í‚¤ì›Œë“œ ìƒì„±ê¸° (Claude Keyword Generator) ---
# ì°¸ê³ : í˜„ì¬ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ì—ì„œëŠ” ì§ì ‘ ì‚¬ìš©ë˜ì§€ ì•Šì§€ë§Œ, ì¶”í›„ í™•ì¥ì„ ìœ„í•´ êµ¬ì¡°ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.
class ClaudeKeywordGenerator:
    """Claudeë¥¼ ì´ìš©í•´ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')
        self.client = anthropic.AsyncAnthropic(api_key=self.api_key) if self.api_key else None
        self.model = "claude-3-opus-20240229"

    async def generate_keywords(self, topic: str, max_keywords: int = 10) -> List[str]:
        """ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.client:
            logger.warning("Anthropic API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ê¸°ë³¸ í‚¤ì›Œë“œ ìƒì„± ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._generate_basic_keywords(topic)

        try:
            prompt = f"""ì£¼ì œ: "{topic}"
ì´ ì£¼ì œì™€ ê´€ë ¨ëœ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ {max_keywords}ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.
- í•µì‹¬ í‚¤ì›Œë“œ 3-4ê°œ
- ê´€ë ¨ ìš©ì–´ 3-4ê°œ
- ìµœì‹  íŠ¸ë Œë“œ í‚¤ì›Œë“œ 2-3ê°œ
í•œ ì¤„ì— í•˜ë‚˜ì”©, í‚¤ì›Œë“œë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
            message = await self.client.messages.create(
                model=self.model,
                max_tokens=500,
                temperature=0.7,
                system="ë‹¹ì‹ ì€ íŠ¹ì • ì£¼ì œì— ëŒ€í•´ ê¹Šì´ ìˆê³  ê´€ë ¨ì„± ë†’ì€ ê²€ìƒ‰ì–´ë¥¼ ì œì•ˆí•˜ëŠ” í‚¤ì›Œë“œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
                messages=[{"role": "user", "content": prompt}]
            )
            keywords = [kw.strip() for kw in message.content[0].text.strip().split('\n') if kw.strip()]
            if topic not in keywords:
                keywords.insert(0, topic)
            return keywords[:max_keywords]
        except Exception as e:
            logger.error(f"Claude í‚¤ì›Œë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return self._generate_basic_keywords(topic)

    def _generate_basic_keywords(self, topic: str) -> List[str]:
        """API ì‚¬ìš©ì´ ë¶ˆê°€ëŠ¥í•  ë•Œ, ê¸°ë³¸ì ì¸ í‚¤ì›Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        return [topic, f"{topic} ìµœì‹ ", f"{topic} ë‰´ìŠ¤", f"{topic} ë™í–¥", f"{topic} ë¶„ì„"]


# --- Perplexity í´ë¼ì´ì–¸íŠ¸ (ê²€ìƒ‰ ë° ê²€ì¦ ê¸°ëŠ¥) ---
class SimplePerplexityClient:
    """Perplexity APIë¥¼ ì‚¬ìš©í•´ ì´ìŠˆë¥¼ ê²€ìƒ‰í•˜ê³ , ê° ì´ìŠˆì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤."""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv('PERPLEXITY_API_KEY')
        if not self.api_key:
            raise ValueError("Perplexity API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        self.base_url = "https://api.perplexity.ai/chat/completions"
        self.model = "sonar-pro"

    # ë³€ê²½ëœ ë¶€ë¶„: time_period ëŒ€ì‹  start_dateì™€ end_dateë¥¼ ë°›ë„ë¡ ìˆ˜ì •
    async def search_issues(self, keywords: List[str], start_date: str, end_date: str) -> Dict[str, Any]:
        """í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§€ì •ëœ ê¸°ê°„ ë™ì•ˆì˜ ê´€ë ¨ ìµœì‹  ì´ìŠˆë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
        prompt = f"""'{", ".join(keywords)}' ê´€ë ¨ ì´ìŠˆë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.

    **ì¤‘ìš”: ë‚ ì§œ ì œì•½ ì¡°ê±´**
    - ë°˜ë“œì‹œ {start_date}ë¶€í„° {end_date}ê¹Œì§€ì˜ ê¸°ê°„ì— ë°œí–‰ëœ ì´ìŠˆë§Œ í¬í•¨í•˜ì„¸ìš”
    - ì´ ê¸°ê°„ ì™¸ì˜ ì´ìŠˆëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
    - ê° ì´ìŠˆì˜ ë°œí–‰ì¼ì´ ìœ„ ê¸°ê°„ ë‚´ì— ìˆëŠ”ì§€ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”

    ì˜¤ëŠ˜ ë‚ ì§œëŠ” {datetime.now().strftime('%Y-%m-%d')}ì…ë‹ˆë‹¤.

    ê° ì´ìŠˆëŠ” ì•„ë˜ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œ ì‘ì„±í•´ì£¼ì„¸ìš”:
    ## **[ì´ìŠˆ ì œëª©]**
    **ìš”ì•½**: [ê°„ë‹¨í•œ ìš”ì•½]
    **ì¶œì²˜**: [ì›¹ì‚¬ì´íŠ¸ëª… ë˜ëŠ” URL]
    **ë°œí–‰ì¼**: [YYYY-MM-DD í˜•ì‹] (ë°˜ë“œì‹œ {start_date} ~ {end_date} ì‚¬ì´ì—¬ì•¼ í•¨)
    **ì¹´í…Œê³ ë¦¬**: [ë‰´ìŠ¤/ê¸°ìˆ /ë¹„ì¦ˆë‹ˆìŠ¤ ë“±]

    ìµœì†Œ 3ê°œ, ìµœëŒ€ 10ê°œì˜ ì´ìŠˆë¥¼ ì°¾ì•„ì£¼ì„¸ìš”. ë‚ ì§œê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚œ ì´ìŠˆëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”."""

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": f"ë‹¹ì‹ ì€ ì •í™•í•œ ì •ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ {start_date}ë¶€í„° {end_date}ê¹Œì§€ì˜ ê¸°ê°„ì— ë°œí–‰ëœ ì´ìŠˆë§Œ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ ê¸°ê°„ ì™¸ì˜ ì •ë³´ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ê° ì´ìŠˆì˜ ë‚ ì§œê°€ ì§€ì •ëœ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ ë°˜ë“œì‹œ í™•ì¸í•˜ê³ , ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ì¶œì²˜ì™€ ì •í™•í•œ ë‚ ì§œë¥¼ í¬í•¨ì‹œì¼œì£¼ì„¸ìš”."
                },
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 4000,
            "temperature": 0.2
        }

        async with httpx.AsyncClient(timeout=60) as client:
            response = await client.post(self.base_url, json=payload, headers=headers)
            response.raise_for_status()
            return response.json()

    async def verify_issue(self, issue: IssueItem) -> VerificationResult:
        """LLMì„ 'judge'ë¡œ í™œìš©í•´, ê° ì´ìŠˆì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤."""
        logger.debug(f"ê²€ì¦ ì‹œì‘: '{issue.title}'")

        prompt = f"""ë‹¹ì‹ ì€ í¸ê²¬ ì—†ëŠ” ì‚¬ì‹¤ í™•ì¸ ì „ë¬¸ê°€(Fact-Checker)ì…ë‹ˆë‹¤. ì•„ë˜ ë‰´ìŠ¤ ì´ìŠˆì˜ ì œëª©, ìš”ì•½, ì¶œì²˜, ë°œí–‰ì¼ì´ ì¼ê´€ë˜ê³  ì‚¬ì‹¤ì¸ì§€ ì›¹ ê²€ìƒ‰ì„ í†µí•´ ê²€ì¦í•´ì£¼ì„¸ìš”.
    [ê²€ì¦í•  ì´ìŠˆ]
    - ì œëª©: {issue.title}
    - ìš”ì•½: {issue.summary}
    - ì¶œì²˜: {issue.source}
    - ë°œí–‰ì¼: {issue.published_date}
    [ì‘ë‹µ í˜•ì‹]
    ì•„ë˜ í˜•ì‹ì„ ë°˜ë“œì‹œ ì§€ì¼œ í•œ ë‹¨ì–´ì˜ íŒê²°ê³¼ í•œ ë¬¸ì¥ì˜ ì´ìœ ë¥¼ ì œì‹œí•˜ì„¸ìš”:
    VERDICT: [VERIFIED, UNVERIFIED, NEEDS_REVIEW ì¤‘ í•˜ë‚˜]
    REASONING: [íŒê²°ì— ëŒ€í•œ ê°„ê²°í•œ ì´ìœ ]"""

        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "You are an impartial fact-checker AI."},
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 300, "temperature": 0.1
        }

        try:
            async with httpx.AsyncClient(timeout=45) as client:
                response = await client.post(self.base_url, json=payload, headers=headers)
                response.raise_for_status()
                content = response.json()['choices'][0]['message']['content']

                # ì‘ë‹µ ë‚´ìš© ë¡œê¹…
                logger.debug(f"ê²€ì¦ API ì‘ë‹µ ('{issue.title}'): {content}")

                verdict_match = re.search(r"VERDICT:\s*(\w+)", content)
                reasoning_match = re.search(r"REASONING:\s*(.*)", content, re.DOTALL)

                status = verdict_match.group(1).strip() if verdict_match else "NEEDS_REVIEW"
                reasoning = reasoning_match.group(1).strip() if reasoning_match else "ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

                # ê²€ì¦ ê²°ê³¼ ìƒì„¸ ë¡œê¹…
                logger.info(f"ê²€ì¦ ê²°ê³¼ - '{issue.title}': {status}")
                logger.debug(f"ê²€ì¦ ì´ìœ : {reasoning}")

                return VerificationResult(status=status, reasoning=reasoning)

        except Exception as e:
            logger.error(f"ì´ìŠˆ ê²€ì¦ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: '{issue.title}', ì˜¤ë¥˜: {e}")
            return VerificationResult(status="NEEDS_REVIEW", reasoning=f"ê²€ì¦ ì¤‘ API ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


# --- ì´ìŠˆ ë¶„ì„ê¸° (Claude Analyzer) ---
class ClaudeAnalyzer:
    """Claudeë¥¼ ì‚¬ìš©í•´ ê²€ì¦ëœ ì´ìŠˆë“¤ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤."""

    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv('ANTHROPIC_API_KEY')
        self.client = anthropic.AsyncAnthropic(api_key=self.api_key) if self.api_key else None
        self.model = "claude-3-opus-20240229"

    async def analyze_issues(self, issues: List[IssueItem], topic: str) -> Dict[str, Any]:
        """ê²€ì¦ëœ ì´ìŠˆ ëª©ë¡ì„ ë°›ì•„ ì „ì²´ì ì¸ íŠ¸ë Œë“œë¥¼ ìš”ì•½í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤."""
        if not self.client or not issues:
            return {"summary": "ë¶„ì„í•  ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.", "full_analysis": ""}

        issues_text = "\n\n".join([f"ì œëª©: {issue.title}\nìš”ì•½: {issue.summary}" for issue in issues[:5]])
        prompt = f"""ì£¼ì œ: "{topic}"
ë‹¤ìŒì€ ì´ ì£¼ì œì™€ ê´€ë ¨í•˜ì—¬ ê²€ì¦ëœ ìµœì‹  ì´ìŠˆ ëª©ë¡ì…ë‹ˆë‹¤:
{issues_text}
ìœ„ ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ ë‹¤ìŒ í•­ëª©ì— ëŒ€í•´ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. ì „ì²´ íŠ¸ë Œë“œ ìš”ì•½ (2-3 ë¬¸ì¥)
2. ì£¼ëª©í•  ë§Œí•œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 3ê°€ì§€
3. í–¥í›„ ì „ë§ (1-2 ë¬¸ì¥)
ê²°ê³¼ëŠ” ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
        try:
            message = await self.client.messages.create(
                model=self.model, max_tokens=1000, temperature=0.5,
                system="ë‹¹ì‹ ì€ ê¸°ìˆ  íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ë‰´ìŠ¤ë¥¼ ì¢…í•©í•´ í•µì‹¬ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤.",
                messages=[{"role": "user", "content": prompt}]
            )
            analysis_text = message.content[0].text
            return {
                "summary": analysis_text.split("\n")[0],
                "full_analysis": analysis_text,
                "analyzed_count": len(issues[:5])
            }
        except Exception as e:
            logger.error(f"Claude ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {"summary": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "full_analysis": ""}


# --- í†µí•© ì´ìŠˆ ê²€ìƒ‰ê¸° (ê²€ìƒ‰, ê²€ì¦, ë¶„ì„ íŒŒì´í”„ë¼ì¸) ---
class ClaudeIssueSearcher:
    """ì´ìŠˆ ê²€ìƒ‰, ê²€ì¦, ë¶„ì„ì˜ ì „ì²´ ê³¼ì •ì„ ê´€ë¦¬í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤."""

    def __init__(self, anthropic_key: Optional[str] = None, perplexity_key: Optional[str] = None):
        self.keyword_generator = ClaudeKeywordGenerator(anthropic_key)
        self.perplexity_client = SimplePerplexityClient(perplexity_key)
        self.analyzer = ClaudeAnalyzer(anthropic_key)

    # ë³€ê²½ëœ ë¶€ë¶„: time_period ëŒ€ì‹  days_agoë¥¼ ë°›ë„ë¡ ìˆ˜ì •
    async def search(self, topic: str, days_ago: int = 7, analyze: bool = True) -> Dict[str, Any]:
        """ì£¼ì–´ì§„ ì£¼ì œì— ëŒ€í•´ ì´ìŠˆë¥¼ ê²€ìƒ‰, ê²€ì¦í•˜ê³ , ì„ íƒì ìœ¼ë¡œ ë¶„ì„ê¹Œì§€ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        start_time = time.time()
        initial_issues, verified_issues = [], []

        # ì¶”ê°€ëœ ë¶€ë¶„: ê²€ìƒ‰ ê¸°ê°„ ê³„ì‚°
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_ago)
        start_date_str = start_date.strftime('%Y-%m-%d')
        end_date_str = end_date.strftime('%Y-%m-%d')

        logger.info(f"ê²€ìƒ‰ ê¸°ê°„: {start_date_str} ë¶€í„° {end_date_str} ê¹Œì§€")

        try:
            # 1ë‹¨ê³„: ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
            logger.info(f"'{topic}'ì— ëŒ€í•œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            keywords = self.keyword_generator._generate_basic_keywords(topic)
            logger.info(f"ìƒì„±ëœ í‚¤ì›Œë“œ: {keywords}")

            # 2ë‹¨ê³„: Perplexityë¡œ 1ì°¨ ì´ìŠˆ ê²€ìƒ‰ (ë³€ê²½ëœ ë¶€ë¶„: ë‚ ì§œ ì¸ì ì „ë‹¬)
            logger.info("Perplexityë¥¼ í†µí•´ 1ì°¨ ì´ìŠˆ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            api_response = await self.perplexity_client.search_issues(keywords, start_date_str, end_date_str)

            # 3ë‹¨ê³„: 1ì°¨ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
            initial_issues = self._parse_response(api_response, keywords)
            if not initial_issues:
                raise ValueError("1ì°¨ ê²€ìƒ‰ ê²°ê³¼, ìœ íš¨í•œ ì´ìŠˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

            # 4ë‹¨ê³„: LLM-as-a-judge í†µí•´ ê° ì´ìŠˆì˜ ì‚¬ì‹¤ ì—¬ë¶€ ê²€ì¦ ë¶€ë¶„ ìˆ˜ì •
            logger.info(f"LLM-as-a-judgeë¥¼ í†µí•´ {len(initial_issues)}ê°œ ì´ìŠˆì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤...")
            verification_tasks = [self.perplexity_client.verify_issue(issue) for issue in initial_issues]
            verification_results = await asyncio.gather(*verification_tasks)

            for issue, verification in zip(initial_issues, verification_results):
                issue.verification = verification
                if verification.status == "VERIFIED":
                    verified_issues.append(issue)
                    logger.success(f"âœ… ê²€ì¦ í†µê³¼: '{issue.title}'")
                else:
                    # ì‹¤íŒ¨/ë³´ë¥˜ ì‹œ ë” ìƒì„¸í•œ ë¡œê¹…
                    logger.warning(f"âŒ ê²€ì¦ ì‹¤íŒ¨/ë³´ë¥˜: '{issue.title}'")
                    logger.warning(f"   ìƒíƒœ: {verification.status}")
                    logger.warning(f"   ì´ìœ : {verification.reasoning}")
                    logger.debug(f"   ì „ì²´ ì´ìŠˆ ì •ë³´: ì œëª©={issue.title}, ì¶œì²˜={issue.source}, ë‚ ì§œ={issue.published_date}")

            # 5ë‹¨ê³„: (ì„ íƒ) ê²€ì¦ëœ ì´ìŠˆë“¤ì„ Claudeë¡œ ì‹¬ì¸µ ë¶„ì„
            analysis = None
            if analyze and verified_issues:
                logger.info("Claudeë¥¼ í†µí•´ ê²€ì¦ëœ ì´ìŠˆë“¤ì˜ ì‹¬ì¸µ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                analysis = await self.analyzer.analyze_issues(verified_issues, topic)

            # 6ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ì •ë¦¬ ë° ë°˜í™˜ (ë³€ê²½ëœ ë¶€ë¶„: ë‚ ì§œ ì •ë³´ ì¶”ê°€)
            search_time = time.time() - start_time
            logger.info(f"ëª¨ë“  ê³¼ì • ì™„ë£Œ. (ì´ ì†Œìš” ì‹œê°„: {search_time:.2f}ì´ˆ)")
            result = SearchResult(
                topic=topic, keywords=keywords, issues=verified_issues,
                initial_found=len(initial_issues), verified_found=len(verified_issues), search_time=search_time,
                search_start_date=start_date_str, search_end_date=end_date_str
            )
            return {"search_result": result, "analysis": analysis}

        except Exception as e:
            logger.error(f"ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "search_result": SearchResult(
                    topic=topic, keywords=[topic], issues=[],
                    initial_found=len(initial_issues), verified_found=0, search_time=time.time() - start_time,
                    search_start_date=start_date_str, search_end_date=end_date_str
                ),
                "analysis": None
            }

    def _parse_response(self, api_response: Dict[str, Any], keywords: List[str]) -> List[IssueItem]:
        """Perplexityì˜ ê²€ìƒ‰ ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ IssueItem ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        try:
            content = api_response['choices'][0]['message']['content']

            # ë””ë²„ê¹…: ì „ì²´ ì‘ë‹µ ë‚´ìš© ì¶œë ¥
            logger.debug(f"Perplexity API ì‘ë‹µ ì „ì²´ ë‚´ìš©:\n{content}\n")

            # ë” ìœ ì—°í•œ ì •ê·œì‹ íŒ¨í„´ë“¤ ì‹œë„
            patterns = [
                r'(?s)(##\s*\*\*.*?(?=\n##\s*\*\*|\Z))',  # ì›ë˜ íŒ¨í„´
                r'(?s)(##\s*\[.*?\].*?(?=\n##\s*\[|\Z))',  # ## [ì œëª©] í˜•ì‹
                r'(?s)(\d+\.\s*\*\*.*?(?=\n\d+\.\s*\*\*|\Z))',  # 1. **ì œëª©** í˜•ì‹
                r'(?s)(\d+\.\s*.*?(?=\n\d+\.|\Z))',  # 1. ì œëª© í˜•ì‹
            ]

            issues = []
            issue_found = False

            for pattern in patterns:
                issue_blocks = list(re.finditer(pattern, content))
                if issue_blocks:
                    logger.info(f"íŒ¨í„´ '{pattern[:30]}...'ë¡œ {len(issue_blocks)}ê°œì˜ ì´ìŠˆ ë¸”ë¡ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                    issue_found = True

                    for match in issue_blocks:
                        section = match.group(1).strip()
                        logger.debug(f"íŒŒì‹± ì¤‘ì¸ ì„¹ì…˜:\n{section[:200]}...")

                        if issue := self._parse_issue_section(section):
                            issue.relevance_score = self._calculate_relevance(issue, keywords)
                            issues.append(issue)
                            logger.info(f"ì´ìŠˆ íŒŒì‹± ì„±ê³µ: '{issue.title}'")
                    break

            if not issue_found:
                logger.warning("ì–´ë–¤ íŒ¨í„´ìœ¼ë¡œë„ ì´ìŠˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì‘ë‹µ í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                # ê°„ë‹¨í•œ í´ë°± íŒŒì‹± ì‹œë„
                issues = self._fallback_parse(content, keywords)

            logger.info(f"ì´ {len(issues)}ê°œì˜ ì´ìŠˆë¥¼ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤.")
            return issues

        except (KeyError, IndexError, AttributeError) as e:
            logger.error(f"API ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ êµ¬ì¡°: {e}")
            logger.error(f"ì‘ë‹µ êµ¬ì¡°: {api_response.keys() if isinstance(api_response, dict) else type(api_response)}")
            return []

    def _fallback_parse(self, content: str, keywords: List[str]) -> List[IssueItem]:
        """ì •ê·œì‹ íŒŒì‹±ì´ ì‹¤íŒ¨í•  ê²½ìš° í´ë°± íŒŒì‹± ë°©ë²•"""
        issues = []

        # ì œëª© ì°¾ê¸° ì‹œë„
        title_patterns = [
            r'ì œëª©:\s*(.+?)(?:\n|$)',
            r'\*\*(.+?)\*\*',
            r'###?\s*(.+?)(?:\n|$)',
            r'\d+\.\s*(.+?)(?:\n|$)'
        ]

        lines = content.split('\n')
        current_issue = {}

        for line in lines:
            line = line.strip()
            if not line:
                continue

            # ìƒˆë¡œìš´ ì´ìŠˆ ì‹œì‘ ê°ì§€
            for pattern in title_patterns:
                match = re.search(pattern, line)
                if match and not any(field in line.lower() for field in ['ìš”ì•½', 'ì¶œì²˜', 'ë°œí–‰ì¼', 'ì¹´í…Œê³ ë¦¬']):
                    if current_issue.get('title'):
                        # ì´ì „ ì´ìŠˆ ì €ì¥
                        issue = self._create_issue_from_dict(current_issue)
                        if issue:
                            issue.relevance_score = self._calculate_relevance(issue, keywords)
                            issues.append(issue)

                    current_issue = {'title': match.group(1).strip()}
                    break

            # í•„ë“œ ì •ë³´ ì¶”ì¶œ
            field_patterns = {
                'ìš”ì•½': r'ìš”ì•½:\s*(.+?)(?:\n|$)',
                'ì¶œì²˜': r'ì¶œì²˜:\s*(.+?)(?:\n|$)',
                'ë°œí–‰ì¼': r'ë°œí–‰ì¼:\s*(\d{4}-\d{2}-\d{2})',
                'ì¹´í…Œê³ ë¦¬': r'ì¹´í…Œê³ ë¦¬:\s*(.+?)(?:\n|$)'
            }

            for field, pattern in field_patterns.items():
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    current_issue[field] = match.group(1).strip()

        # ë§ˆì§€ë§‰ ì´ìŠˆ ì €ì¥
        if current_issue.get('title'):
            issue = self._create_issue_from_dict(current_issue)
            if issue:
                issue.relevance_score = self._calculate_relevance(issue, keywords)
                issues.append(issue)

        return issues

    def _create_issue_from_dict(self, issue_dict: Dict[str, str]) -> Optional[IssueItem]:
        """ë”•ì…”ë„ˆë¦¬ë¡œë¶€í„° IssueItem ìƒì„±"""
        if not issue_dict.get('title'):
            return None

        return IssueItem(
            title=issue_dict['title'],
            summary=issue_dict.get('ìš”ì•½', issue_dict['title']),
            source=issue_dict.get('ì¶œì²˜', 'Unknown'),
            published_date=issue_dict.get('ë°œí–‰ì¼'),
            category=issue_dict.get('ì¹´í…Œê³ ë¦¬', 'general'),
            relevance_score=0.5
        )

    def _parse_issue_section(self, section: str) -> Optional[IssueItem]:
        """ê°œë³„ ì´ìŠˆ ë¸”ë¡ì„ íŒŒì‹±í•˜ì—¬ IssueItem ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        try:
            title_match = re.search(r'##\s*\*\*(.*?)\*\*', section)
            if not title_match: return None
            title = title_match.group(1).strip()
            summary = self._extract_field(section, 'ìš”ì•½')
            source = self._extract_field(section, 'ì¶œì²˜') or 'Unknown'
            date_str = self._extract_field(section, 'ë°œí–‰ì¼')
            category = self._extract_field(section, 'ì¹´í…Œê³ ë¦¬') or 'general'
            url_match = re.search(r'https?://[^\s)]+', source)
            url = url_match.group(0) if url_match else None
            return IssueItem(title=title, summary=summary or title, source=source, published_date=date_str, relevance_score=0.5, category=category, url=url)
        except Exception as e:
            logger.error(f"ê°œë³„ ì´ìŠˆ ì„¹ì…˜ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def _extract_field(self, text: str, field_name: str) -> Optional[str]:
        """ì •ê·œì‹ì„ ì‚¬ìš©í•´ íŠ¹ì • í•„ë“œì˜ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        pattern = rf'\*\*{field_name}\*\*:\s*(.*?)(?=\n\*\*|\Z)'
        match = re.search(pattern, text, re.DOTALL)
        return match.group(1).strip() if match else None

    def _calculate_relevance(self, issue: IssueItem, keywords: List[str]) -> float:
        """ì´ìŠˆì˜ ì œëª©ê³¼ ìš”ì•½ì— í‚¤ì›Œë“œê°€ ì–¼ë§ˆë‚˜ í¬í•¨ë˜ì—ˆëŠ”ì§€ë¡œ ê´€ë ¨ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        text = f"{issue.title} {issue.summary}".lower()
        score = sum(1.0 - (i * 0.1) for i, keyword in enumerate(keywords) if keyword.lower() in text)
        return round(min(score / len(keywords), 1.0) if keywords else 0.0, 2)


# --- ë©”ì¸ ì‹¤í–‰ ë¡œì§ ---
async def main():
    """ìŠ¤í¬ë¦½íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ì…ë‹ˆë‹¤."""
    if not os.getenv('PERPLEXITY_API_KEY') or not os.getenv('ANTHROPIC_API_KEY'):
        print("â›”ï¸ ì˜¤ë¥˜: .env íŒŒì¼ì— 'PERPLEXITY_API_KEY'ì™€ 'ANTHROPIC_API_KEY'ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        return

    searcher = ClaudeIssueSearcher()
    # ê²€ìƒ‰í•  ì£¼ì œì–´ì…ë‹ˆë‹¤.
    topic = "iOS"
    # ë³€ê²½ëœ ë¶€ë¶„: ì˜¤ëŠ˜ë¡œë¶€í„° ë©°ì¹  ì „ê¹Œì§€ì˜ ì´ìŠˆë¥¼ ê²€ìƒ‰í• ì§€ ìˆ«ìë¡œ ì§€ì •í•©ë‹ˆë‹¤. (ì˜ˆ: 30ì€ ìµœê·¼ 30ì¼)
    days_to_search = 30

    result = await searcher.search(topic, days_ago=days_to_search, analyze=True)

    search_result = result["search_result"]
    analysis = result["analysis"]
    output_filename = f"{topic.replace(' ', '_')}_ì´ìŠˆ_ë¶„ì„_ê²°ê³¼.txt"
    original_stdout = sys.stdout

    try:
        with open(output_filename, "w", encoding="utf-8") as f:
            sys.stdout = f

            print(f"ì£¼ì œ: '{topic}'ì— ëŒ€í•œ ì´ìŠˆ ë¶„ì„ ë³´ê³ ì„œ\n" + "="*50)
            # ì¶”ê°€ëœ ë¶€ë¶„: ê²€ìƒ‰ ê¸°ê°„ì„ ëª…ì‹œì ìœ¼ë¡œ ì¶œë ¥
            print(f"âœ”ï¸ ê²€ìƒ‰ ê¸°ê°„: {search_result.search_start_date} ~ {search_result.search_end_date} ({days_to_search}ì¼ê°„)")
            print(f"âœ”ï¸ ì‚¬ìš©ëœ í‚¤ì›Œë“œ: {', '.join(search_result.keywords)}")
            print(f"âœ”ï¸ 1ì°¨ ê²€ìƒ‰ëœ ì´ìŠˆ: {search_result.initial_found}ê°œ")
            print(f"âœ”ï¸ ìµœì¢… ê²€ì¦ëœ ì´ìŠˆ: {search_result.verified_found}ê°œ (ì‹ ë¢°ë„ í–¥ìƒ)")
            print(f"âœ”ï¸ ì´ ì†Œìš” ì‹œê°„: {search_result.search_time:.2f}ì´ˆ\n")

            if not search_result.issues:
                print("ìµœì¢…ì ìœ¼ë¡œ ê²€ì¦ëœ ì´ìŠˆê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                print(f"--- ìƒìœ„ {min(5, len(search_result.issues))}ê°œ ê²€ì¦ëœ ì´ìŠˆ ìƒì„¸ ì •ë³´ ---\n")
                for i, issue in enumerate(search_result.issues[:5], 1):
                    print(f"{i}. {issue.title}")
                    print(f"   - ì¶œì²˜: {issue.source}")
                    print(f"   - ë‚ ì§œ: {issue.published_date}")
                    print(f"   - ê´€ë ¨ë„: {issue.relevance_score:.1%}")
                    print(f"   - ê²€ì¦: {issue.verification.status} ({issue.verification.reasoning})")
                    print(f"   - ìš”ì•½: {issue.summary}\n")

            if analysis and analysis.get('full_analysis'):
                print("\n" + "--- Claude ì‹¬ì¸µ ë¶„ì„ ê²°ê³¼ ---\n" + "="*50)
                print(analysis['full_analysis'])

        sys.stdout = original_stdout
        print(f"\nğŸ‰ ì„±ê³µ! ë¶„ì„ ê²°ê³¼ê°€ '{output_filename}' íŒŒì¼ì— ì•ˆì „í•˜ê²Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        sys.stdout = original_stdout
        logger.error(f"ê²°ê³¼ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"â›”ï¸ ì˜¤ë¥˜: ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ({e})")


if __name__ == "__main__":
    logger.remove()
    logger.add(sys.stderr, format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>", level="DEBUG")

    print("ğŸš€ ì§€ëŠ¥í˜• ì´ìŠˆ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("âš ï¸ Perplexity APIëŠ” ì§§ì€ ì‹œê°„ì— ë§ì€ ìš”ì²­ì„ ë³´ë‚´ë©´ ì‚¬ìš©ëŸ‰ ì œí•œì— ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜í•˜ì„¸ìš”.")

    asyncio.run(main())